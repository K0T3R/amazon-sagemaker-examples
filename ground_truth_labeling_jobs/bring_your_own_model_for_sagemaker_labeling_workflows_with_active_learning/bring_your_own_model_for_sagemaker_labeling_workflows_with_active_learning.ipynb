{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Resources Needed for an Active Learning Workflow\n",
    "\n",
    "Use this notebook to create the resources required to create an automated labeling workflow for a text-classification labeling job. Specifically, we will create:\n",
    "\n",
    "* An input manifest file using the UCI News Dataset with 20% of the data labeled\n",
    "* A CreateLabelingJob request\n",
    "\n",
    "**This notebook is intended to be used along side the blog post [Bring your own model for SageMaker labeling workflows with Active Learning](https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-for-amazon-sagemaker-labeling-workflows-with-active-learning/),  Part 1: Create an Active Learning Workflow with BlazingText**.\n",
    "\n",
    "While following along with this blog post, we recommend that you leave most of the cells unmodified. However, the notebook will indicate where you can modify variables to create the resources needed for a custom labeling job.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "To run the code in this notebook, you will need to: \n",
    "\n",
    "* Create an AWS account.\n",
    "* Create an IAM role with required permissions. For simplicity, add policies to your role to grant full access to S3, Step Functions, Amazon SageMaker, CloudWatch and CloudFormation. If required, add more granular permissions while ensuring your role has access to the resources created and used throughout this demo. \n",
    "\n",
    "If you plan to customize the Ground Truth labeling job request configuration below, you will also need the resources required to create a labeling job. For more information, see [Use Amazon SageMaker Ground Truth for Data Labeling](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html). \n",
    "\n",
    "### Using this Notebook\n",
    "\n",
    "Run the code cells in this notebook to configure a Labeling Job request in JSON format. This request JSON can be used in an active learning workflow and will determine how your labeling job task appears to human workers. \n",
    "\n",
    "To customize this notebook, you will need to modify the the cells below and configure the Ground Truth labeling job request (`human_task_config`) to meet your requirements. To learn how to create a Ground Truth labeling job using the Amazon SageMaker API, see [CreateLabelingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateLabelingJob.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,sagemaker, tensorflow as tf, pandas as pd, boto3, numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sess.boto_session.region_name\n",
    "bucket= sess.default_bucket(); key='sagemaker-byoal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labeling input manifest file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an input manifest file for our active learning workflow using the newsCorpora.csv file from the [UCI News Dataset](https://archive.ics.uci.edu/ml/datasets/News+Aggregator). This dataset contains a list of about 420,000 articles that fall into one of four categories: Business (b), Science & Technology (t), Entertainment (e) and Health & Medicine (m). We will randomly choose 10,000 articles from that file to create our dataset.\n",
    "\n",
    "For the active learning loop to start, 20% of the data must be labeled. To quickly test the active learning component, we will include 20% (`labeled_count`) of the original labels provided in the dataset in our input manifest. We use this partially-labeled dataset as the input to the active learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-02 02:17:58--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "WARNING: cannot verify archive.ics.uci.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
      "  Issued certificate has expired.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29224203 (28M) [application/x-httpd-php]\n",
      "Saving to: ‘NewsAggregatorDataset.zip’\n",
      "\n",
      "NewsAggregatorDatas 100%[===================>]  27.87M  32.6MB/s    in 0.9s    \n",
      "\n",
      "2020-04-02 02:17:59 (32.6 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
      "\n",
      "Archive:  NewsAggregatorDataset.zip\n",
      "  inflating: 2pageSessions.csv       \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._2pageSessions.csv  \n",
      "  inflating: newsCorpora.csv         \n",
      "  inflating: __MACOSX/._newsCorpora.csv  \n",
      "  inflating: readme.txt              \n",
      "  inflating: __MACOSX/._readme.txt   \n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip --no-check-certificate && unzip NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "manifest_file = \"partially-labeled.manifest\"\n",
    "news_data_all = pd.read_csv('newsCorpora.csv', names=column_names, header=None, delimiter='\\t')\n",
    "news_data = news_data_all.sample(n=10000)\n",
    "news_data = news_data[[\"TITLE\",\"CATEGORY\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will clean our data set using *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"TITLE\"].replace('\"','',inplace=True,regex=True)\n",
    "news_data[\"TITLE\"].replace('[^\\w\\s]','',inplace=True,regex=True)\n",
    "news_data[\"TITLE\"] = news_data[\"TITLE\"].str.split('\\n').str[0]\n",
    "news_data['CATEGORY'] = news_data['CATEGORY'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = news_data[\"TITLE\"].str.lower().replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.to_csv(\"news_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will create our partially-labeled input manifest file, and push it to our S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "total=len(news_data)\n",
    "labeled_count = int(total / 5) #20% of the dataset is labeled.\n",
    "label_map = {\n",
    "             \"b\": \"Business\",\n",
    "             \"e\": \"Entertainment\",\n",
    "             \"m\": \"Health & Medicine\",\n",
    "             \"t\": \"Science and Technology\"\n",
    "          }\n",
    "labeled_series=pd.Series(data=news_data.iloc[:labeled_count].TITLE.values,index=news_data.iloc[:labeled_count].CATEGORY.values)\n",
    "annotation_metadata = b\"\"\"{ \"category-metadata\" : { \"confidence\": 1.0, \"human-annotated\": \"yes\", \"type\": \"groundtruth/text-classification\"} }\"\"\"\n",
    "annotation_metadata_dict = json.loads(annotation_metadata)\n",
    "with open(manifest_file, 'w') as outfile:\n",
    "    for items in labeled_series.iteritems():\n",
    "        labeled_record = dict()\n",
    "        labeled_record[\"source\"] = items[1]\n",
    "        labeled_record[\"category\"] =  int(items[0])\n",
    "        labeled_record.update(annotation_metadata_dict)\n",
    "        outfile.write(json.dumps(labeled_record) + \"\\n\")\n",
    "\n",
    "unlabeled_series=pd.Series(data=news_data.iloc[labeled_count:].TITLE.values,index=news_data.iloc[labeled_count:].CATEGORY.values)\n",
    "with open(manifest_file, 'a') as outfile:\n",
    "    for items in unlabeled_series.iteritems():\n",
    "        outfile.write(\"{\\\"source\\\":\\\"\"+items[1]+\"\\\"}\\n\")    \n",
    "    \n",
    "boto3.resource('s3').Bucket(bucket).upload_file(manifest_file,key+ \"/\" + manifest_file)\n",
    "manifest_file_uri =  \"s3://{}/{}\".format(bucket,key+ \"/\" + manifest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use s3 client to upload relevant json strings to s3.\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will specify the labels that workers will use to categorize the articles. To customize your labeling job, add your own labels here. To learn more, see [LabelCategoryConfigS3Uri](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateLabelingJob.html#sagemaker-CreateLabelingJob-request-LabelCategoryConfigS3Uri)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file_name = \"class_labels.json\"\n",
    "label_file = \"\"\"{\n",
    "    \"document-version\": \"2018-11-28\",\n",
    "    \"labels\": [\n",
    "        {\n",
    "            \"label\": \"Business\"\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Entertainment\"\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Health & Medicine\"\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Science and Technology\"\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "s3_client.put_object(Body=label_file, Bucket=bucket, Key=key+ \"/\" + label_file_name)\n",
    "label_file_uri =  \"s3://{}/{}\".format(bucket,key+ \"/\" + label_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will specify our custom worker task template. This template will configure the UI that workers will see when they open our text classification labeling job tasks. To learn how to customize this cell, see  [Creating your custom labeling task template](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_file_name = \"instructions.template\"\n",
    "template_file = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "<crowd-form>\n",
    "  <crowd-classifier\n",
    "    name=\"crowd-classifier\"\n",
    "    categories=\"{{ task.input.labels | to_json | escape }}\"\n",
    "    header=\"Select the news title corresponding to the 4 categories. (b) for Business, (e) for Entertainment, (m) for Health and Medicine and (t) for Science and Technology.\"\n",
    "  >\n",
    "    <classification-target> {{ task.input.taskObject }} </classification-target>\n",
    "    <full-instructions header=\"Classifier instructions\">\n",
    "      <ol><li><strong>Read</strong> the text carefully.</li><li><strong>Read</strong> the examples to understand more about the options.</li><li><strong>Choose</strong> the appropriate label that best suits the text.</li></ol>\n",
    "    </full-instructions>\n",
    "    <short-instructions>\n",
    "      <p>Example Business title:</p><p>US open: Stocks fall after Fed official hints at accelerated tapering.</p><p><br>\n",
    "      </p><p>Example Entertainment title:</p><p>CBS negotiates three more seasons for The Big Bang Theory</p><p><br>\n",
    "      </p><p>Example Health & Medicine title:</p><p>Blood Test Could Predict Alzheimer's. Good News? </p><p><br>\n",
    "      </p><p>Example Science and Technology (t) title:</p><p>Elephants tell human friend from foe by voice.</p><p><br>\n",
    "      </p>\n",
    "    </short-instructions>\n",
    "  </crowd-classifier>\n",
    "</crowd-form>\n",
    "<!-- TODO: remove this before pushing -->\n",
    "<!--htacbotcategories:Business,Entertainment,Health & Medicine,Science and Technology --> \n",
    "\"\"\"\n",
    "\n",
    "s3_client.put_object(Body=template_file, Bucket=bucket, Key=key+ \"/\" + template_file_name)\n",
    "template_file_uri =  \"s3://{}/{}\".format(bucket,key+ \"/\" + template_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a private work team to labeling your data objects, set `USE_PRIVATE_WORKFORCE` to `True` and input your work team ARN for `private_workteam_arn`. You must have a private workforce in the same AWS Region as your labeling job task request to use a private work team. To learn more see [Use a Private Workforce](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-private.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRIVATE_WORKFORCE = False\n",
    "private_workteam_arn = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will automatically configure a public workforce ARN and pre- and post-annotation ARNs (`prehuman_arn` and `acs_arn` respectively). If `USE_PRIVATE_WORKFORCE` is `False` a public workforce will be used to create your labeling job request. \n",
    "\n",
    "To customize your labeling job task type, you will need to modify `prehuman_arn` and `acs_arn`. \n",
    "\n",
    "If you are using one of the Ground Truth built-in task types, you can find pre- and post-annotation lambda ARNs using the following links. \n",
    "* Pre-annotation lambda ARNs for built in task types can be found in [HumanTaskConfig](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HumanTaskConfig.html#API_HumanTaskConfig_Contents).\n",
    "* Post-annotation lambda ARNs (Annotation Consolidation Lambda) for built in task types can be found in [AnnotationConsolidationConfig](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AnnotationConsolidationConfig.html#sagemaker-Type-AnnotationConsolidationConfig-AnnotationConsolidationLambdaArn).\n",
    "\n",
    "If you are creating a custom labeling job task, see [Step 3: Processing with AWS Lambda\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step3.html) learn how to create custom pre- and post-annotation lambda ARNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify ARNs for resources needed to run a text classification job.\n",
    "ac_arn_map = {'us-west-2': '081040173940',\n",
    "              'us-east-1': '432418664414',\n",
    "              'us-east-2': '266458841044',\n",
    "              'eu-west-1': '568282634449',\n",
    "              'ap-northeast-1': '477331159723'}\n",
    "\n",
    "public_workteam_arn = 'arn:aws:sagemaker:{}:394669845002:workteam/public-crowd/default'.format(region)\n",
    "prehuman_arn = 'arn:aws:lambda:{}:{}:function:PRE-TextMultiClass'.format(region, ac_arn_map[region])\n",
    "acs_arn = 'arn:aws:lambda:{}:{}:function:ACS-TextMultiClass'.format(region, ac_arn_map[region])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell specifies our labeling job name, the description works see, and tags that workers can use to find our labeling job task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = \"byoal-news\"\n",
    "task_description = 'Classify news title to one of these 4 categories.'\n",
    "task_keywords = ['text', 'classification', 'humans', 'news']\n",
    "task_title = task_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the following request to customize your labeling job request. For more information on the parameters below, see [CreateLabelingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateLabelingJob.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_task_config = {\n",
    "      \"AnnotationConsolidationConfig\": {\n",
    "        \"AnnotationConsolidationLambdaArn\": acs_arn,\n",
    "      },\n",
    "      \"PreHumanTaskLambdaArn\": prehuman_arn,\n",
    "      \"MaxConcurrentTaskCount\": 200, # 200 texts will be sent at a time to the workteam.\n",
    "      \"NumberOfHumanWorkersPerDataObject\": 1, # 1 workers will be enough to label each text.\n",
    "      \"TaskAvailabilityLifetimeInSeconds\": 21600, # Your work team has 6 hours to complete all pending tasks.\n",
    "      \"TaskDescription\": task_description,\n",
    "      \"TaskKeywords\": task_keywords,\n",
    "      \"TaskTimeLimitInSeconds\": 300, # Each text must be labeled within 5 minutes.\n",
    "      \"TaskTitle\": task_title,\n",
    "      \"UiConfig\": {\n",
    "        \"UiTemplateS3Uri\": template_file_uri,\n",
    "      }\n",
    "    }\n",
    "\n",
    "if not USE_PRIVATE_WORKFORCE:\n",
    "    human_task_config[\"PublicWorkforceTaskPrice\"] = {\n",
    "        \"AmountInUsd\": {\n",
    "           \"Dollars\": 0,\n",
    "           \"Cents\": 1,\n",
    "           \"TenthFractionsOfACent\": 2,\n",
    "        }\n",
    "    } \n",
    "    human_task_config[\"WorkteamArn\"] = public_workteam_arn\n",
    "else:\n",
    "    human_task_config[\"WorkteamArn\"] = private_workteam_arn\n",
    "\n",
    "ground_truth_request = {\n",
    "        \"InputConfig\" : {\n",
    "          \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "              \"ManifestS3Uri\": manifest_file_uri,\n",
    "            }\n",
    "          },\n",
    "          \"DataAttributes\": {\n",
    "            \"ContentClassifiers\": [\n",
    "              \"FreeOfPersonallyIdentifiableInformation\",\n",
    "              \"FreeOfAdultContent\"\n",
    "            ]\n",
    "          },  \n",
    "        },\n",
    "        \"OutputConfig\" : {\n",
    "          \"S3OutputPath\": 's3://{}/{}/output/'.format(bucket, key),\n",
    "        },\n",
    "        \"HumanTaskConfig\" : human_task_config,\n",
    "        \"LabelingJobNamePrefix\": job_name_prefix,\n",
    "        \"RoleArn\": role, \n",
    "        \"LabelAttributeName\": \"category\",\n",
    "        \"LabelCategoryConfigS3Uri\": label_file_uri,\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(ground_truth_request, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the following steps to trigger the Active Learning loop.\n",
    "\n",
    "1.\tOpen the AWS Step Functions console: http://console.aws.amazon.com/states\n",
    "2.\tThe Cloud Formation stack provided in the blog post has generated two step function in the State Machines section: **ActiveLearningLoop-*** and **ActiveLearning-*** where * will be replaced with the name you used when you launched your Cloud Formation stack. \n",
    "3.\tSelect ActiveLearningLoop-*. \n",
    "4.\tChoose **Start Execution**.\n",
    "5.\tPaste the JSON above in **Input – optional code-block**.\n",
    "6.\tSelect **Start execution**. \n",
    "\n",
    "    \n",
    "These manual steps could be automated by using the data science SDK. Please refer to the details [here](https://aws.amazon.com/about-aws/whats-new/2019/11/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On successful completion of the active learning loop, the state machine will output the final output manifest file and the latest trained model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bring Your Own Model to an Active Learning Workflow \n",
    "\n",
    "Use this notebook to learn how to containerize your own Machine Learning model and push it to [Amazon Elastic Container Registry (ERC)](https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html). This notebook will produce an ECR ID that you can use to integrate your model into an active learning workflow.\n",
    "\n",
    "**This notebook is intended to be used along side the blog post [Bring your own model for Amazon SageMaker labeling workflows with Active Learning](https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-for-amazon-sagemaker-labeling-workflows-with-active-learning/), Part 2: Create a Custom Model and Integrate it into an Active Learning Workflow**.\n",
    " \n",
    "\n",
    "### To Use this Notebook\n",
    "\n",
    "We use this notebook to tokenize our dataset and create a training dataset, add a containerized model to ERC, and train the model. The notebook will produce an image name in ECR which can be used for training and inference across Amazon SageMaker. \n",
    "\n",
    "We use a Keras deep learning model for demonstration purposes only. The methodology for developing and containerizing our model was inspired by the tutorial [Take an ML from idea to production using Amazon SageMaker](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification) and is not included in the notebook. \n",
    "\n",
    "To customize this notebook, you will need to create your own machine learning model and add it to a Docker container. Use the blog post above to learn how to do this with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will set up our environment and extract our account number. We will use the account number to define an image name for the Elastic Container Repository (ECR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region = sess.boto_session.region_name\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/news-classifier'.format(account, region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Tokenizing the data\n",
    "\n",
    "First we read the csv news dataset using pandas and clean the data:\n",
    "\n",
    "* We make all alphanumeric characters lowercase and replace undesired characters. \n",
    "* We remove stop words and empty records. \n",
    "\n",
    "The result is saved into a JSON formatted file.\n",
    "\n",
    "Next, we use the [Keras Tokenizer class](https://keras.io/preprocessing/text/) to tokenize our dataset and upload it to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stop_words=stop_words.ENGLISH_STOP_WORDS\n",
    "import os,sys,sagemaker, tensorflow as tf, pandas as pd, boto3, numpy as np\n",
    "\n",
    "column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "tf_train = pd.read_csv('newsCorpora.csv', names=column_names, header=None, delimiter='\\t')\n",
    "train_s3_key = 'sagemaker/newsCorpora.csv'\n",
    "boto3.resource('s3').Bucket(bucket).upload_file('newsCorpora.csv',train_s3_key)\n",
    "\n",
    "tf_train= tf_train[[\"TITLE\",\"CATEGORY\"]]\n",
    "tf_train[\"TITLE\"]=tf_train[\"TITLE\"].str.lower().replace('[^\\w\\s]','')\n",
    "tf_train[\"TITLE\"]= tf_train[\"TITLE\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "tf_train.dropna(inplace=True)\n",
    "\n",
    "cat=tf_train['CATEGORY'].astype(\"category\").cat.categories\n",
    "tf_train['CATEGORY']=tf_train['CATEGORY'].astype(\"category\").cat.codes\n",
    "y=tf_train['CATEGORY'].values\n",
    "\n",
    "\n",
    "max_features=5000 #we set maximum number of words to 5000\n",
    "maxlen=100 #and maximum sequence length to 100\n",
    "embedding_dim = 50 #this is the final dimension of the embedding space.\n",
    "tok = tf.keras.preprocessing.text.Tokenizer(num_words=max_features) #tokenizer step\n",
    "tok.fit_on_texts(list(tf_train['TITLE'])) #fit to cleaned text\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "boto3.resource('s3').Bucket(bucket).upload_file('tokenizer.pickle',key+'/tokenizer.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the first 1000 entries for training and add them to a manifest file. Then, we save our training manifest file in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "train_file = \"train-manifest\"\n",
    "tf_train = pd.read_csv('newsCorpora.csv', names=column_names, header=None, delimiter='\\t')\n",
    "tf_train= tf_train[[\"TITLE\",\"CATEGORY\"]]\n",
    "tf_train[\"TITLE\"]=tf_train[\"TITLE\"].str.replace('\"','').replace('\\r', '')\n",
    "tf_train['CATEGORY']=tf_train['CATEGORY'].astype(\"category\").cat.codes\n",
    "\n",
    "series=pd.Series(data=tf_train.iloc[:1000].TITLE.values,index=tf_train.iloc[:1000].CATEGORY.values)\n",
    "with open(train_file, 'w') as outfile:\n",
    "    for items in series.iteritems():\n",
    "        outfile.write(\"{\\\"category\\\":\"+str(items[0])+\",\\\"source\\\":\\\"\"+items[1]+\"\\\"}\\n\")\n",
    "    \n",
    "    \n",
    "boto3.resource('s3').Bucket(bucket).upload_file(train_file,key+ \"/\" + train_file)\n",
    "train_s3_uri =  \"s3://{}/{}\".format(bucket,key+ \"/\" + train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Containerized ML Model to ECR\n",
    "\n",
    "The next cell will create a repository in ECR (if it does not exist already), build our docker image locally, and then [push it to ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\n",
      "Starting docker:\t.[  OK  ]\n",
      "Sending build context to Docker daemon  56.83kB\n",
      "Step 1/7 : FROM tensorflow/tensorflow:latest-py3\n",
      " ---> 53187075965b\n",
      "Step 2/7 : RUN apt-get update &&     apt-get install -y nginx\n",
      " ---> Using cache\n",
      " ---> 6b4999ccca34\n",
      "Step 3/7 : RUN pip install gevent gunicorn flask sagemaker-containers pandas s3fs sklearn\n",
      " ---> Using cache\n",
      " ---> f9a05bcdf34b\n",
      "Step 4/7 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> e3dbac68ac94\n",
      "Step 5/7 : ENV PYTHONUNBUFFERED=1\n",
      " ---> Using cache\n",
      " ---> 5fcc9d55f40a\n",
      "Step 6/7 : COPY news-classifier /opt/program\n",
      " ---> f9f7065284f3\n",
      "Step 7/7 : WORKDIR /opt/program\n",
      " ---> Running in 39e5b128f150\n",
      "Removing intermediate container 39e5b128f150\n",
      " ---> 3f078f3a0388\n",
      "Successfully built 3f078f3a0388\n",
      "Successfully tagged news-classifier:latest\n",
      "The push refers to repository [166163396559.dkr.ecr.us-west-2.amazonaws.com/news-classifier]\n",
      "101198849a61: Preparing\n",
      "8cfe76ba6002: Preparing\n",
      "c261f482a42a: Preparing\n",
      "bbd58cb63302: Preparing\n",
      "fb8dafd6f834: Preparing\n",
      "b0f840119697: Preparing\n",
      "d3d0f89939b4: Preparing\n",
      "2f9b4f91799b: Preparing\n",
      "74bb2e59dcc5: Preparing\n",
      "0ea2b2c9ed16: Preparing\n",
      "918efb8f161b: Preparing\n",
      "27dd43ea46a8: Preparing\n",
      "9f3bfcc4a1a8: Preparing\n",
      "2dc9f76fb25b: Preparing\n",
      "b0f840119697: Waiting\n",
      "d3d0f89939b4: Waiting\n",
      "2f9b4f91799b: Waiting\n",
      "74bb2e59dcc5: Waiting\n",
      "0ea2b2c9ed16: Waiting\n",
      "918efb8f161b: Waiting\n",
      "27dd43ea46a8: Waiting\n",
      "9f3bfcc4a1a8: Waiting\n",
      "2dc9f76fb25b: Waiting\n",
      "bbd58cb63302: Layer already exists\n",
      "fb8dafd6f834: Layer already exists\n",
      "8cfe76ba6002: Layer already exists\n",
      "c261f482a42a: Layer already exists\n",
      "b0f840119697: Layer already exists\n",
      "d3d0f89939b4: Layer already exists\n",
      "2f9b4f91799b: Layer already exists\n",
      "74bb2e59dcc5: Layer already exists\n",
      "0ea2b2c9ed16: Layer already exists\n",
      "27dd43ea46a8: Layer already exists\n",
      "918efb8f161b: Layer already exists\n",
      "9f3bfcc4a1a8: Layer already exists\n",
      "2dc9f76fb25b: Layer already exists\n",
      "101198849a61: Pushed\n",
      "latest: digest: sha256:645162a924f2219a15fc0a6d2170339e659bccbcd6652d818b57904bbdb3de63 size: 3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=news-classifier\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x ${algorithm_name}/train\n",
    "chmod +x ${algorithm_name}/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Model\n",
    "\n",
    "We train our model on the training data that we extracted above and see the accuracy returned by our algorithm in Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmp0pojz2kf_algo-1-oyxqw_1 ... \n",
      "\u001b[1BAttaching to tmp0pojz2kf_algo-1-oyxqw_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m   warnings.warn(message, FutureWarning)\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:26.742204: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:26.742927: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:26.742953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m Starting the training with input_path /opt/ml/input/data\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:31.816519: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:31.816555: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:31.816588: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0bcda3bac843): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:31.816777: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:31.823007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300080000 Hz\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:31.823574: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a41140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m 2020-02-26 22:02:31.823605: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m Train on 750 samples, validate on 250 samples\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m Epoch 1/3\n",
      "750/750 [==============================] - 1s 2ms/sample - loss: 0.4164 - accuracy: 0.9573 - val_loss: 1.1277 - val_accuracy: 0.7760\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m Epoch 2/3\n",
      "750/750 [==============================] - 1s 718us/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.5092 - val_accuracy: 0.7760\n",
      "\u001b[36malgo-1-oyxqw_1  |\u001b[0m Epoch 3/3\n",
      "750/750 [==============================] - 1s 795us/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.6411 - val_accuracy: 0.7760\n",
      "\u001b[36mtmp0pojz2kf_algo-1-oyxqw_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator = Estimator(image_name= 'news-classifier',\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='local')\n",
    "\n",
    "estimator.fit(train_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the Image name in ECR\n",
    "\n",
    "The cell below will pring our image's name in ECR. This image can now be used for both training and inference across Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166163396559.dkr.ecr.us-west-2.amazonaws.com/news-classifier\n"
     ]
    }
   ],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add this image to an active learning workflow follow the instructions in *Step 1: Update the container ECR reference* in the blog. \n",
    "\n",
    "The active learning workflow resources produced by the Cloud Formation Stack provided in **Bring your own model for SageMaker labeling workflows with Active Learning** defaults to a `MultiRecord` [batch strategy](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#sagemaker-CreateTransformJob-request-BatchStrategy). If your model only support a `SingleRecord` batch strategy, change your batch strategy by following the instructions in *Step 2: Change batch strategy*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
